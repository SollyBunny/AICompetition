<link rel="stylesheet" href="./global.css">
<h3> Latent Diffusion for Unique Terrain Generation </h3>

<h4> tldr; </h4>
Terrain generation in games often uses noise to generate natural looking terrain. More often than not, this results in very monotonus terrain which although can be beautiful, isn't very interesting. Any unique features such as biomes, landforms and structures must be hard coded in (such as in Minecraft and City Skylines). Instead you could use a relativley new AI technique, latent diffusion (more commonly known as stable diffusion), to generate much more interesting and unique terrain with little to no human input. If this works, it could create realistic terrain along with unrealstic but extremley intersting terrain. For example if trained on data containing landforms such as <a href="https://en.wikipedia.org/wiki/Giant%27s_Causeway">Gaint's Causeway</a>, it could attempt to make similar but very different geometric patterns. A large scale end goal could be to train the AI multiple times on different parts of the world, and then stitch the output together to create an alternate but eerily similar Earth, this could also be done for other terrestial bodies such as the moon and mars.

<h4> Why </h4>
Many <a href="https://en.wikipedia.org/wiki/AAA_(video_game_industry)">Triple A games</a> have <i>huge</i> open word maps, some of these include <a href="https://en.wikipedia.org/wiki/Grand_Theft_Auto">GTA</a>, <a href="https://en.wikipedia.org/wiki/Cyberpunk_2077"> Cyberpunk 2077</a> and <a href="https://en.wikipedia.org/wiki/No_Man%27s_Sky">No Man's Sky</a>. A lot of time is spent building these and they often end up very <a href="https://www.merriam-webster.com/dictionary/homogeneous">homogeneous</a>. They are also limited by size as they are often big 3d models, it could be possible to generate, on the fly an open world map as a sort of <a href="https://en.wikipedia.org/wiki/Just-in-time_compilation">JIT</a> open world game. It would also be an introduction for me in Latent Diffusion, as it is very different from conventional Neurel Networks, as it happens over multiple time steps.

<h4> What is Latent Diffusion </h4>
<a href="https://en.wikipedia.org/wiki/Diffusion_model">Latent Diffusion</a> (aka: Stable Diffusion) is a method to reverse the processes of diffusion (blurring) either with human designed algorithms, or in this case AI. This can be used for denoising, superresolution and generation. It does these by being trained on clear and noisy images. This is normally done by extracting smaller squares of pixels out of the image instead of attempting to do the whole image at once, this reduces the networks' size aswell as reduce data duplication. This does have a major flaw, which is that the AI is blind to data further than a few pixels away which means non organic patterns (such as text) are incredibly hard. This is somewhat the opposite to wave collapse where you start from nothing and generate graphs (a set of interconnected points) by collapsing a wave function for each point using (again) either a human generated algorithm or AI. Although Latent Diffusion is relativly new in terms of being available to the public it has proved extremley usefull and versitile being able to <a href="https://thispersondoesnotexist.com/">create faces</a> (note this is actually a <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GAN</a>), perform text to image (<a href="https://stablediffusionweb.com/#demo">Stable Diffusion</a>, <a href="https://www.midjourney.com/">Midjourney</a> [which, although closed source, is by far the best for static image gen <a href="./midjourney.html">Examples</a>] and <a href="https://openai.com/dall-e-2/">OpenAI Dall-E 2</a>), image inpainting (<a href="https://www.nvidia.com/research/inpainting/index.html">NVidia</a>) and as of very recently, video generation (<a href="https://youtu.be/YxmAQiiHOkA">Two Minute Papers [good channel]</a>)

<h4> How to exploit Latent Diffusion for Terrain Gen </h4>
As Latent Diffusion is an emerging method, there is no general purpose libraries or many pre-trained general-purpose networks for the task (like there is for face detection or motion tracking). This means I either have to write my own, or try to coerse a pre-existing one (probably Stable Diffusion due to it being <a href="https://en.wikipedia.org/wiki/Free_and_open-source_software">FOSS</a>) to generate terrain instead of imagery (very precise <a href="https://en.wikipedia.org/wiki/Prompt_engineering">Prompt Engineering</a> as well as messing with the tuning variables). If I were to write my own, options such as <a href="https://en.wikipedia.org/wiki/TensorFlow">TensorFlow</a> would probably be preferable to a ground up implementation. I have tried many times (1 [lost in disk drive, racing], <a href="https://github.com/sollybunny/ai-racing">2</a> [racing, multicore/thread/cpu], <a href="https://github.com/sollybunny/ainet">3</a> [wip, general purpose]), and failed somewhat horribly. TensorFlow can run on <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">GPU</a>s, CPUs and <a href="https://en.wikipedia.org/wiki/Cloud_computing">on the cloud</a>, all at once, as well as being extremley optimized for production usage and of course, it does pretty much everything for you.

<h4> Training Data </h4>
Deciding, and finding the training data is probably the most important part of a neurel network as any change in it will determine everything about the output. You need enough data to prevent <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> (when the data is "memorised" by the neurel network and is later <a href="https://en.wikipedia.org/wiki/Regurgitation_(digestion)"regurgitated</a>). This is especially hard when the only really suitiable map data would come from <a href="https://developers.google.com/maps/documentation/javascript/overview">Google's data</a> (which is a heightmap along with many 2d images) and <a href="https://wiki.openstreetmap.org/wiki/API">Open Street View</a>. Both these data sets also do not contain the many interesting features such as valleys, caves, overhangs, crop cicles, craters, etc in the heightmap and processing the 2d images would take an extreme amount of time. It would also be better to have the data in a <a href="https://en.wikipedia.org/wiki/Volume_rendering">volumetric form</a> (rather than mesh [which is trivial to get from heightmaps]), as it would be easier to process at the end (where the AI and the user can set <a href="https://en.wikipedia.org/wiki/Constraint_programming">constraints</a>) and it is easier for the AI to generate a volume rather than a mesh (meshes are extremley counter-intuitive for an AI, and will probably cause the AI to generate an illogical mess of triangles). The easiest way to generate training data would be to literally generate terrain with conventional methods then feed it in and coerse (through <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">Reinforcement Learning</a>) and constrain the AI to try to make it more unique, I could also use a game like <a href="https://en.wikipedia.org/wiki/Minecraft">Minecraft</a> (using a library such as <a href="https://github.com/maxogden/minecraft-mca">minecraft-mca</a> and attempt to parse it into a usable form. If this was possible, terrain generation mods could also be included in the dataset (<a href="https://www.curseforge.com/minecraft/mc-mods/simplex-terrain-generation">Simplex Terrain Generation</a> and <a href="https://www.curseforge.com/minecraft/mc-mods/terralith">Terralith</a>, more can be found at <a href="https://www.curseforge.com/minecraft/mc-mods/world-gen">Curseforge</a>). Generating terrain with <a href="https://en.wikipedia.org/wiki/Perlin_noise">Perlin Noise</a> (<a href="https://github.com/wwwtyro/perlin.js/">JS Perlin Noise Library</a>) is also possible. There are also full 3d models of cities such as <a href="https://www.nyc.gov/site/planning/data-maps/open-data/dwn-nyc-3d-model-download.page">New York City</a>. All of these data have their pros and cons, but all can be used at the same time (and in different proportions).

<h4> Output </h4>
The most important part of any project is seeing the results. Obviously I would prefer if the output could be 3D, but there's no reason why 2D isn't viable. There are many mediums I can use to display the data such as Minecraft, <a href="https://threejs.org/">three.js</a> and <a href="https://en.wikipedia.org/wiki/Polygon_mesh">mesh</a> models (which could then be viewed in any 3d viewer such as <i><a href="https://en.wikipedia.org/wiki/Paint_3D">Paint 3D</a></i>). I also need to consider textures and colors, because either the AI could generate them, or could be added afterwards as a post processing step.
